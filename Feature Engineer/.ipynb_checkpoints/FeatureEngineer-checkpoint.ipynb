{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cf3fd918",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd,numpy as np,matplotlib.pyplot as plt, seaborn as sns,warnings,statistics,scipy.stats as stats,pylab\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.under_sampling import NearMiss\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.combine import SMOTETomek\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler,RobustScaler,OneHotEncoder,LabelEncoder\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba0594a",
   "metadata": {},
   "source": [
    "# TRAIN, VALIDATION, TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e88646e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Train,Valid,Test = np.split(df.sample(frac=1), [int(0.6*len(df)),int(0.8*len(df))])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e708e6",
   "metadata": {},
   "source": [
    "# DATA MINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b939d26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Data_Mining(df):\n",
    "    numerical = [feature for feature in df.columns if df[feature].dtype != 'O']\n",
    "    discrete = [feature for feature in numerical if df[feature].nunique()<25]\n",
    "    continuous = [feature for feature in numerical if feature not in discrete]\n",
    "    categorical = [feature for feature in df.columns if df[feature].dtype=='O']\n",
    "    return numerical,discrete,continuous,categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4425f3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Feature_NAN(df):\n",
    "    numerical,discrete,continuous,categorical = Data_Mining(df)\n",
    "    na_numerical = [feature for feature in numerical if df[feature].isnull().sum()>0]\n",
    "    na_discrete = [feature for feature in discrete if df[feature].isnull().sum()>0]\n",
    "    na_continuous = [feature for feature in continuous if df[feature].isnull().sum()>0]\n",
    "    na_categorical = [feature for feature in categorical if df[feature].isnull().sum()>0]\n",
    "    return na_numerical,na_discrete,na_continuous,na_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34463163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# numerical,discrete,continuous,categorical = Data_Mining(X)\n",
    "# na_numerical,na_discrete,na_continuous,na_categorical = Feature_NAN(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b11c291",
   "metadata": {},
   "source": [
    "# 1. MISSING VALUE\n",
    "1.1. NUMERICAL\n",
    "+ DISCRETE BEST WAY IS MODE\n",
    "+ CONTINUOUS BEST WAY IS MEDIAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b26aa6f",
   "metadata": {},
   "source": [
    "1.1.1. MEAN, MODE, MEDIAN IMPUTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5deabb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Replace_measure_numerical(df,features,impute):\n",
    "    data = df.copy()\n",
    "    for feature in features:\n",
    "        if impute =='median':\n",
    "            value = data[feature].median()\n",
    "        elif impute =='mean':\n",
    "            value = data[feature].mean()\n",
    "        else:\n",
    "            value = statistics.mode(data[feature])\n",
    "        data[feature] = data[feature].fillna(value)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca48c81",
   "metadata": {},
   "source": [
    "1.1.2. RANDOM SAMPLE IMPUTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf665b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Replace_randomsample_numerical(df,features):\n",
    "    data = df.copy()\n",
    "    for feature in features:\n",
    "        random_sample = data[feature].dropna().sample(data[feature].isnull().sum(),random_state=0)\n",
    "        random_sample.index = data[data[feature].isnull()].index\n",
    "        data.loc[data[feature].isnull(),feature] = random_sample\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c677ed",
   "metadata": {},
   "source": [
    "1.1.3. END OF DISTRIBUTION IMPUTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "794eb29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for feature in na_continuous:\n",
    "#     plt.figure(figsize=(10,2))\n",
    "#     sns.distplot(df_hp[feature])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c2432bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Replace_end_numerical(df,features,location):\n",
    "    data = df.copy()\n",
    "    for feature in features:\n",
    "        if location == 'left':\n",
    "            extreme = data[feature].mean()-3*data[feature].std()\n",
    "        else:\n",
    "            extreme = data[feature].mean()+3*data[feature].std()\n",
    "        \n",
    "        data[feature] = data[feature].fillna(extreme)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c0c455",
   "metadata": {},
   "source": [
    "1.1.4. ARBITRAY IMPUTATION (0,-1,99,-100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d4b76b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Replace_arbitray_numerical(df,features,value):\n",
    "    data = df.copy()\n",
    "    data[features] = data[features].fillna(value)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb8ae3c",
   "metadata": {},
   "source": [
    "1.1.5. BEFORE (pad/ffill) OR AFTER(bfill/backfill)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "88405c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Replace_befor_after_numerical(df,features,method,limit=2):\n",
    "    data = df.copy()\n",
    "    data[features] = data[features].fillna(method=method,limit=limit)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2261af18",
   "metadata": {},
   "source": [
    "1.1.6. INTERPOLATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e3715439",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Replace_interpolate_numerical(df,features,method,order):\n",
    "    data = df.copy()\n",
    "    data[features] = data[features].interpolate(method=method,order=order)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "38c9aa01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_feature_engineer = Replace_measure_numerical(df,na_numerical,'median')\n",
    "# df_feature_engineer = Replace_randomsample_numerical(df,na_numerical)\n",
    "# df_feature_engineer = Replace_end_numerical(df,na_continuous,'right')\n",
    "# df_feature_engineer = Replace_arbitray_numerical(df,na_numerical,-99)\n",
    "# df_feature_engineer = Replace_befor_after_numerical(df,na_numerical,'bfill')\n",
    "# df_feature_engineer = Replace_interpolate_numerical(df,na_numerical,'polynomial',5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5ac7c6",
   "metadata": {},
   "source": [
    "1.2. CATEGORICAL (less nan values: Mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107786bc",
   "metadata": {},
   "source": [
    "1.2.1. MISSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ff8b7534",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Replace_missing_cat(df,features,value):\n",
    "    data = df.copy()\n",
    "    data[features] = data[features].fillna(value)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ecbdc25",
   "metadata": {},
   "source": [
    "1.2.2. FREQUENT CATEGORICAL (LESS NAN VALUES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6e4b3ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Replace_frequent_cat(df,features):\n",
    "    data = df.copy()\n",
    "    for feature in features:\n",
    "        frequent = data[feature].value_counts().index[0]\n",
    "        data[feature] = data[feature].fillna(frequent)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fafce54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_feature_engineer = Replace_missing_cat(df,na_categorical,'Missing')\n",
    "# df_feature_engineer = Replace_frequent_cat(df,na_categorical)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4c34a2",
   "metadata": {},
   "source": [
    "1.2.3. TRAIN/TEST "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ed51fa",
   "metadata": {},
   "source": [
    "1.2.4. UNSUPERVISED ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8a2341a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Replace_kmeans_cat(df,feature):\n",
    "    data = df.copy()\n",
    "    train_cluster = data.dropna(axis=0).drop(columns=feature)\n",
    "    kmeans = KMeans(n_clusters = data[feature].nunique()).fit(train_cluster)\n",
    "    data.loc[data[feature].notnull(),feature] = kmeans.labels_\n",
    "    test_cluster = data.dropna(axis=1).loc[data[data[feature].isnull()].index,]\n",
    "    data.loc[data[feature].isnull(),feature] = kmeans.predict(test_cluster)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cd9f25c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = Replace_kmeans_cat(X_train,'KJ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d429b9d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2665f4f",
   "metadata": {},
   "source": [
    "# 2. ENCODING\n",
    "+ CHANGE CATEGORICAL VARIABLES LESS THAN 1% EQUAL 'RARE VAR'\n",
    "+ FOUND WHICH FEATURE IS HIGH CARDINALITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f42bc41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Rare_Var(df,features):\n",
    "    data = df.copy()\n",
    "    for feature in features:\n",
    "        temp = data.groupby(feature)[feature].count()/len(data)\n",
    "        temp_df = temp[temp>0.01].index\n",
    "        data[feature] = np.where(data[feature].isin(temp_df),data[feature],'Rare Var')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1fd14118",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Cardinality(df,categorical):\n",
    "    high_cardinality = [feature for feature in categorical if df[feature].nunique()>15]\n",
    "    normal_cardinality = [feature for feature in categorical if df[feature].nunique()>5 and feature not in high_cardinality]\n",
    "    low_cardinality = [feature for feature in categorical if feature not in high_cardinality+normal_cardinality]\n",
    "    return high_cardinality,normal_cardinality,low_cardinality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f70b0cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_feature_engineer = Rare_Var(X,categorical)\n",
    "# high_cardinality,normal_cardinality,low_cardinality = Cardinality(X,categorical)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cad3a75",
   "metadata": {},
   "source": [
    "2.1. Normial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906be2e7",
   "metadata": {},
   "source": [
    "2.1.1. HOT ENCODING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a433ee09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def One_hot_encoding(df,features):\n",
    "    data = df.copy()\n",
    "    for feature in features:\n",
    "        encoder = OneHotEncoder(drop='first')\n",
    "        encoded_rank = encoder.fit_transform(data[feature].values.reshape(-1,1)).toarray()\n",
    "        encoded_rank = pd.DataFrame(encoded_rank,columns=feature+'_'+data[feature].value_counts().index.sort_values()[1:])\n",
    "        data = pd.concat([data,encoded_rank],axis=1)\n",
    "        data.drop(columns=feature,inplace=True)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3aabdf2",
   "metadata": {},
   "source": [
    "2.1.2. TOP 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "35c51305",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TOP_encoding(df,features,top):\n",
    "    data = df.copy()\n",
    "    for feature in features:\n",
    "        top_x = [label for label in data[feature].value_counts().head(top).index]\n",
    "        for label in top_x:\n",
    "            data[feature+'_'+str(label)] = np.where(data[feature]==label,1,0)\n",
    "        data.drop(columns=feature,inplace=True)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1cb081",
   "metadata": {},
   "source": [
    "2.1.3. COUNT/FREQUENT CATEGORICAL ENCODING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "efaa9fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Count_encoding(df,features):\n",
    "    data = df.copy()\n",
    "    for feature in features:\n",
    "        data[feature+'_'+'count'] = data[feature].map(data[feature].value_counts())\n",
    "        data.drop(columns=feature,inplace=True)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24767f1c",
   "metadata": {},
   "source": [
    "2.1.4. MEAN ENCODING (TARGET: BINARY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "190adb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Mean_encoding(df,features,target):\n",
    "    data = df.copy()\n",
    "    for feature in features:\n",
    "        data[feature+'_'+'mean_target'] = data[feature].map(data.groupby([feature])[target].mean())\n",
    "        data.drop(columns=feature,inplace=True)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0a5b1d",
   "metadata": {},
   "source": [
    "2.1.5. PROBABILITY RATIO ENCODING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7c9390c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Probability_ratio_encoding(df,features,target):\n",
    "    data = df.copy()\n",
    "    for feature in features:\n",
    "        prob = data.groupby([feature])[target].mean()\n",
    "        data[feature+'_'+'prob_ratio'] = data[feature].map(prob/(1-prob))\n",
    "        data.drop(columns=feature,inplace=True)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c6de9160",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_feature_engineer = One_hot_encoding(X,low_cardinality)\n",
    "# df_feature_engineer = TOP_encoding(X,high_cardinality,top=10)\n",
    "# df_feature_engineer = Count_encoding(X,normal_cardinality)\n",
    "# df_feature_engineer = Mean_encoding(X,categorical,y.name)\n",
    "# df_feature_engineer = Probability_ratio_encoding(X,categorical,y.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dff241b",
   "metadata": {},
   "source": [
    "2.2. ORDINAL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2319261b",
   "metadata": {},
   "source": [
    "2.2.1. LABEL ENCODING\n",
    "+ from sklearn.preprocessing import LabelEncoder: this transformer should be used to encode TARGET VALUES, i.e. y, and not the INPUT X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "db9ea0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Label_encoding_target(target):\n",
    "    label_encoder = LabelEncoder()\n",
    "    target = label_encoder.fit_transform(pd.DataFrame(target))\n",
    "    return target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "573ea2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Label_encoding_sklearn(X_train, X_test):\n",
    "    ordinal_encoder = OrdinalEncoder()\n",
    "    ordinal_encoder.fit(X_train)\n",
    "    X_train = ordinal_encoder.transform(X_train)\n",
    "    X_test = ordinal_encoder.transform(X_test)\n",
    "    return X_train,X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "19668877",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Label_encoding(df,features):\n",
    "    data = df.copy()\n",
    "    for feature in features:\n",
    "        labels_sort = data[feature].value_counts().index.sort_values()\n",
    "        labels_ordinal = {label:ordinal for ordinal,label in enumerate(labels_sort,1)}\n",
    "        data[feature+'_'+'label'] = data[feature].map(labels_ordinal)\n",
    "        data.drop(columns=feature,inplace=True)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbacdd76",
   "metadata": {},
   "source": [
    "2.2.2. TARGET GUIDED ORDINAL ENCODING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "00865e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Target_guided_encoding(df,features,target):\n",
    "    data = df.copy()\n",
    "    for feature in features:\n",
    "        labels_sort = data.groupby([feature])[target].mean().sort_values().index\n",
    "                                        \n",
    "        labels_ordinal = {label:ordinal for ordinal,label in enumerate(labels_sort,1)}\n",
    "                                                                  \n",
    "        data[feature+'_'+'target_ordinal'] = data[feature].map(labels_ordinal)\n",
    "        data.drop(columns=feature,inplace=True)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3311a45",
   "metadata": {},
   "source": [
    "2.2.3. COMBINE COLUMNS (relationship nonlinear between feature and feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd4a4d0",
   "metadata": {},
   "source": [
    "2.2.4. BINS (AGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "80e3c001",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Bins_label(df,features):\n",
    "    data = df.copy()\n",
    "    for feature in features:\n",
    "        labels = data[feature]\n",
    "        bins =  np.select([labels<10,labels<20,labels<30,labels<40,labels<50,labels<60],[0,1,2,3,4,5],6)\n",
    "        data['Age'+'_'+'bins'] = pd.DataFrame(bins)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4860b74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y = Label_encoding_target(y)\n",
    "# X_train,X_test = Label_encoding_sklearn(X_train, X_test)\n",
    "# df_feature_engineer = Label_encoding(X,low_cardinality)\n",
    "# df_feature_engineer = Target_guided_encoding(X,categorical,y.name)\n",
    "# df_feature_engineer = Bins_label(X,categorical)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3f7445",
   "metadata": {},
   "source": [
    "# 3. IMBALANCED\n",
    "3.1. Under Sampling\n",
    "+ Lấy số lượng thiểu số tổng thể chia tỷ lệ rồi làm tròn xuống thì bằng số lượng mẫu đa số (Tỷ lệ cân bằng phải cao hơn ban đầu)\n",
    "+ Số lượng mẫu lớn sau under phải cao hơn n_neighbors (mặc định bằng 3: có thể thay đổi)\n",
    "+ Các feature đều là biến số\n",
    "3.2. Over Sampling\n",
    "+ Không cần tất cả là number\n",
    "+ Lấy số lượng đa số tổng thể nhân tỉ lệ rồi làm tròn xuống thì ra số lượng mẫu thiểu số (mẫu thiểu số lấy từ tổng thể thiểu số đã có)\n",
    "+ Tỷ lệ cân bằng phải cao hơn ban đầu\n",
    "3.3. SMOTETomek\n",
    "+ Cần tất cả là number\n",
    "+ Tổng thể đa số giảm, tổng thể thiểu số giảm và tăng thêm điểm hoàn toàn mới (nằm trong vùng tổng thể thiểu số)\n",
    "+ Số lượng mẫu đa số nhân tỷ lệ rồi làm tròn ra số lượng mẫu thiểu số\n",
    "+ Tỷ lệ % đa số cũ giảm xuống tỷ lệ % đa số mới bao nhiêu thì tỷ lệ % thiểu số cũ tăng lên tỷ lệ % thiểu số mới là bấy nhiêu sao cho số lượng đa số mới nhân tỷ lệ rồi làm tròn ra số lượng thiểu số mới."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7687a778",
   "metadata": {},
   "source": [
    "3 biến phân loại đầu ra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f03e3c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# strategy = {0:5699, 1:5699, 2:5699}\n",
    "# oversample = SMOTETomek(sampling_strategy=strategy)\n",
    "# X, y = oversample.fit_resample(X_feature_engineer,y_feature_engineer)\n",
    "# X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.33,random_state=42)\n",
    "# Train_scale,test_scale = Scale(StandardScaler(),X_train,X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6af48279",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Imbalanced(X,y,ratio,method):\n",
    "    if method == 'undersampling':\n",
    "        X_balanced,y_balanced = NearMiss(ratio).fit_resample(X,y)\n",
    "    elif method == 'oversampling':\n",
    "        X_balanced,y_balanced = RandomOverSampler(ratio).fit_resample(X,y)\n",
    "    else:\n",
    "        X_balanced,y_balanced = SMOTETomek(ratio).fit_resample(X,y)\n",
    "    return X_balanced,y_balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e146abc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train,y_train = Imbalanced(X,y,0.95,'SMOTETomek')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6835ca49",
   "metadata": {},
   "source": [
    "# 4. Treating Outliers\n",
    "+ Using scatter plots\n",
    "+ Box plot\n",
    "+ Using z-core: nếu dữ liệu có phân phối chuẩn\n",
    "+ Using IQR (Interquartile Range): nếu không có phân phối chuẩn\n",
    "# IQR & ZSCORE:\n",
    "   + Triming ouliers\n",
    "   + Capping outlier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c154ce",
   "metadata": {},
   "source": [
    "4.1. IQR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cfb1f5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def IQR_method(df,features,method):\n",
    "    data = df.copy()\n",
    "    for feature in features:\n",
    "        q1 = data[feature].quantile(0.25)\n",
    "        q3 = data[feature].quantile(0.75)\n",
    "        iqr = q3 - q1\n",
    "        upper_whisker = q3 + (1.5*iqr)\n",
    "        lower_whisker = q1 - (1.5*iqr)\n",
    "        points = data[feature]\n",
    "        if method == 'Capping':\n",
    "            data[feature] = np.where(points>upper_whisker, upper_whisker,np.where(points<lower_whisker, lower_whisker, points))\n",
    "        else:\n",
    "            drop_outliers = np.array([])\n",
    "            indexes = points[(points > upper_whisker)|(points<lower_whisker)].index\n",
    "            drop_outliers = np.append(drop_outliers,indexes)\n",
    "            dropped = np.unique(drop_outliers)\n",
    "            data = data.drop(labels=dropped)\n",
    "    data.reset_index(inplace=True)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d4a21b",
   "metadata": {},
   "source": [
    "4.2. Zscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5f56b2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Zscore_method(df,features,thresh,method):\n",
    "    data = df.copy()\n",
    "    for feature in features:\n",
    "        mean = data[feature].mean()\n",
    "        std = data[feature].std()\n",
    "        upper_bound = mean + thresh*std\n",
    "        lower_bound = mean - thresh*std\n",
    "        points = data[feature]\n",
    "        if method == 'Capping':\n",
    "            data[feature] = np.where(points>upper_bound,upper_bound,np.where(points<lower_bound,lower_bound, points))\n",
    "        else:\n",
    "            drop_outliers = np.array([])\n",
    "            indexes = points[(points > upper_bound)|(points<lower_bound)].index\n",
    "            drop_outliers = np.append(drop_outliers,indexes)\n",
    "            dropped = np.unique(drop_outliers)\n",
    "            data = data.drop(labels=dropped)\n",
    "    data.reset_index(drop=True,inplace=True)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8d775ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_feature_engineer = IQR_method(X,discrete,'Triming')\n",
    "# df_feature_engineer = Zscore_method(X,continuous,thresh = 3,'Triming')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6ce753",
   "metadata": {},
   "source": [
    "# 5. SCALING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ebf751",
   "metadata": {},
   "source": [
    "5.1. Check Skewness: Q-Q plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "891a6d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def QQ_plot(df,features):\n",
    "    for feature in features:\n",
    "        fig,axs = plt.subplots(1,3,figsize=(20,6))\n",
    "        plt.rcParams['font.family']='Arial'\n",
    "        plt.rcParams['font.size']=13\n",
    "        sns.set_style('dark')\n",
    "        sns.distplot(df[feature], label=\"skew: \" + str(np.round(df[feature].skew(),2)),ax = axs[0])\n",
    "        axs[0].legend()\n",
    "        sns.boxplot(df[feature],ax=axs[2])\n",
    "        axs[2].set_title('Box Plot')\n",
    "        stats.probplot(df[feature],dist='norm',plot=axs[1])\n",
    "        axs[0].set_title('Distribution')\n",
    "        fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "32172f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Distribution(df):\n",
    "    normal_feature = [feature for feature in df.columns if np.abs(np.round(df[feature].skew(),2)) < 0.5]\n",
    "    skew_feature = [feature for feature in df.columns if np.abs(np.round(df[feature].skew(),2)) >= 0.5]\n",
    "    return normal_feature,skew_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3a493708",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QQ_plot(X,continuous)\n",
    "# normal_feature, skew_feature = Distribution(X[numerical])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6202981c",
   "metadata": {},
   "source": [
    "5.2. Not gaussian disrtribution\n",
    "+ If you want to check whether feature is guassian or normal distributed: Q-Q plot\n",
    "+ #If value equal 0 use np.log1p (add 1 for all point)\n",
    "+ BoxCox Transformation\n",
    "+ The Box-Cox transformation is defined as: T(Y)=(Y exp(λ)-1)/λ\n",
    "\n",
    "+ where Y is the response variable and λ is the transformation parameter. λ varies from -5 to 5. In the transformation, all values of λ are considered and the optimal value for a given variable is selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e914fe93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Gaussion_transformation(types,features,df):\n",
    "    data = df.copy()\n",
    "    for feature in features:\n",
    "        if types == 'Logarithmic' and 0 in data[feature].unique():\n",
    "            data[feature] = np.log1p(data[feature])\n",
    "        elif types == 'Logarithmic' and 0 not in data[feature].unique():\n",
    "            data[feature] = np.log(data[feature])\n",
    "        elif types == 'Reciprocal' and 0 not in data[feature].unique():\n",
    "            data[feature] = np.divide(1,data[feature])\n",
    "        elif types == 'SquareRoot' and data[feature].unique()>=0:\n",
    "            data[feature] = np.sqrt(data[feature])\n",
    "        elif types == 'Exponential':\n",
    "            data[feature] = np.power(data[feature],2)\n",
    "        elif types == 'BoxCox':\n",
    "            data[feature],parameters = stats.boxcox(data[feature])\n",
    "        else:\n",
    "            data[feature] = data[feature]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06feb5dd",
   "metadata": {},
   "source": [
    "5.3. Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8a181b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Scale(types,train,test):\n",
    "    scaler = types\n",
    "    df_train = pd.DataFrame(scaler.fit_transform(train),columns=train.columns)\n",
    "    df_test = pd.DataFrame(scaler.transform(test),columns=test.columns)\n",
    "    return df_train,df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d497f6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_feature_engineer = Gaussion_transformation('Logarithmic',skew_feature,X)\n",
    "# X_train_scale,X_test_scale = Scale(StandardScaler(),X,X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7417a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def scale_dataset(df,oversample=False):\n",
    "#     x = df[df.cols[:-1]].values\n",
    "#     y = df[df.cols[-1]].values\n",
    "#     scaler = StandardScaler()\n",
    "#     X = scaler.fit_transform(X)\n",
    "#     if oversample:\n",
    "#         ros = RandomOverSampler()\n",
    "#         X,y = ros.fit_resample(X,y)\n",
    "#     data np.hstack((X,np.reshape(y,(-1,1))))\n",
    "#     return data,X,y\n",
    "# train, X_train, y_train = scale_dataset(train,oversample=True)\n",
    "# valid, X_valid, y_valid = scale_dataset(valied,oversample=True)\n",
    "# test, X_test, y_test = scale_dataset(test,oversample=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
